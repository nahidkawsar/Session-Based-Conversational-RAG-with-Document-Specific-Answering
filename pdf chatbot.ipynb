{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pip install langchain_groq\n",
    "- pip install langchain_huggingface\n",
    "- pip install sentence-transformers\n",
    "- pip install onnxruntime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 2: Load Environment Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Fetch the API keys from environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Initialize the Language Model (LLM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000002000F16B7D0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000002000CADCD90>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# Initialize the Groq language model\n",
    "llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")\n",
    "llm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 4: Set Up Hugging Face Embeddings for Text Representation**\n",
    "\n",
    "Install and configure Hugging Face embeddings using sentence-transformers to convert text into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Create HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 5: Load and Process Web Content**\n",
    "\n",
    "You can scrape content from a web page and process it into chunks for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pypdf in f:\\generative_ai\\end_to_end_projects\\app _first\\chatbot  with external web infomation\\myenv\\lib\\site-packages (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "## Reading a PDf File\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader=PyPDFLoader('Resume Of Nahid kawsar.pdf')\n",
    "docs=loader.load()\n",
    "docs\n",
    "\n",
    "\n",
    "# Load documents from the web\n",
    "docs = loader.load()\n",
    "\n",
    "# Split the content into smaller chunks for indexing\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Now 'splits' will contain the documents in smaller chunks ready for embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 6: Set Up Chroma for Vector Storage**\n",
    "Use Chroma to store and retrieve document embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x000002000C53CD10>, search_kwargs={})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Chroma is used to store the embeddings for retrieval\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 7:ChatPromptTemplate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Define the system prompt\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "# Create the chat prompt template\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 8: Create a Retrieval Chain for Q&A**\n",
    "\n",
    "Now that the content is indexed, we create a chain for querying the information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `create_stuff_documents_chain` function combines a set of documents (usually retrieved by a retriever) \n",
    "# into a prompt to generate an answer.Mainly used when we need to combine multiple documents.\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `create_stuff_documents_chain` function is used to create a chain that generates answers by combining the retrieved documents' context and the \n",
    "# language model (LLM).It takes two parameters: 1. `llm`: The language model and 2. `prompt`: A `ChatPromptTemplate` that defines the structure of the \n",
    "# prompt used by the LLM.The `question_answer_chain` will now contain a callable chain that combines the context from retrieved \n",
    "# documents (passed as part of the prompt) and processes it using the language model to generate answers.\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `create_retrieval_chain` function is used to create a Retrieval-Augmented Generation (RAG) chain.It takes two parameters:\n",
    "# 1. `retriever`: The retriever is responsible for retrieving relevant documents from the data store based on the input query.\n",
    "# 2. `question_answer_chain`: This is the chain that will generate an answer based on the retrieved documents (from the `retriever`).\n",
    "\n",
    "# The `rag_chain` is a combination of a retriever and a question-answering chain, where:\n",
    "# - The retriever first searches for the most relevant documents.\n",
    "# - These documents are then passed into the `question_answer_chain`, which processes them and generates an answer.\n",
    "# \n",
    "# This creates a pipeline where the system retrieves relevant information before generating an answer, making it more contextually aware.\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is H.M.Nahid Kawsar.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Query the system with a question\n",
    "response = rag_chain.invoke({\"input\": \"What is my name\"})\n",
    "\n",
    "# Step 4: Retrieve and print the answer from the response\n",
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 10: Manage Session-Based Chat History**\n",
    "\n",
    "We can store chat history by session ID, which helps to maintain a session for ongoing interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The `conversational_rag_chain.invoke()` function is used to execute the conversational retrieval-augmented generation (RAG) chain.It takes two main arguments:\n",
    "# 1. `{\"input\": \"What is Task Decomposition?\"}`: The input query that the user asks. In this case, it's asking about \"Task Decomposition\".\n",
    "# 2. `config={ \"configurable\": {\"session_id\": \"abc123\"} }`: The session ID is used to store and retrieve the chat history specific to this session. \n",
    "#    This allows for a personalized conversation where the context (previous conversation) is remembered between queries.\n",
    "#\n",
    "# The `session_id` ensures that the conversation can continue with the correct context, making it \"conversational.\"\n",
    "# The response will contain the answer generated based on the current query and the context of the previous chat history stored under the session ID \"abc123\".\n",
    "\n",
    "response = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"chat_01\"}  # This ensures the session is tracked and history is maintained\n",
    "    }\n",
    ")[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Session-Based Conversational RAG with Document-Specific Answering Using Retrieval, Chat History, and Language Models (LLM), Applied to PDFs, Websites, and Other Documents**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**document-based answering capabilities.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The person's name is H.M. Nahid Kawsar.\""
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what is his name?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"chat_01\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nahid's university name is Noakhali Science and Technology University (NSTU).\""
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what is nahids university name?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"chat_01\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the context, it seems that Nahid Kawsar is a dedicated and passionate learner, committed to continuous development and employability enhancement. He has demonstrated exceptional commitment and adaptability in the \"21st Century\\'s Employability Skills Development Course\" and has achieved over 70% in every assessment.'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"what do you think about Nahid kawsar\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"chat_01\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
